{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn ##########\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix,recall_score,matthews_corrcoef,roc_curve,roc_auc_score,auc,precision_recall_curve\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "\n",
    "#----->>\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "# from scipy import interp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_bicoding(seq,wsize=41):\n",
    "\t#return bicoding for a sequence\n",
    "\tseq=seq.replace('U','T') #turn rna seq to dna seq if have\n",
    "\tfeat_bicoding=[]\n",
    "\tbicoding_dict={'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],'T':[0,0,0,1],'N':[0,0,0,0]}\n",
    "\tif len(seq) < wsize:\n",
    "\t\tseq=seq+'N'*(wsize-len(seq))\n",
    "\tfor each_nt in seq:\n",
    "\t\tfeat_bicoding+=bicoding_dict[each_nt]\n",
    "\treturn feat_bicoding # (404,) list\n",
    "\n",
    "\n",
    "def load_data_bicoding(Path):\n",
    "\tdata = np.loadtxt(Path,dtype=list)\n",
    "\tdata_result = []\n",
    "\tfor seq in data:\n",
    "\t\tseq = str(seq.strip('\\n'))\n",
    "\t\tbicoding=convert_seq_to_bicoding(seq)\n",
    "\t\tdata_result.append(bicoding)\n",
    "\treturn data_result \n",
    "\n",
    "\n",
    "\n",
    "def load_train_val_bicoding(pos_train_fa,neg_train_fa):\n",
    "\tdata_pos_train = []\n",
    "\tdata_neg_train = []\n",
    "\t\n",
    "\tdata_pos_train = load_data_bicoding(pos_train_fa)\n",
    "\tdata_neg_train = load_data_bicoding(neg_train_fa)\n",
    "\t\n",
    "\t\n",
    "\tdata_train = np.array([_ + [1] for _ in data_pos_train] + [_ + [0] for _ in data_neg_train])\n",
    "\tnp.random.seed(42)\n",
    "\tnp.random.shuffle(data_train)\n",
    "\t\n",
    "\tX = np.array([_[:-1] for _ in data_train])\n",
    "\ty = np.array([_[-1] for _ in data_train])\n",
    "\t\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1.0/10, random_state=42)\n",
    "\t\n",
    "\treturn X_train,y_train,X_test,y_test\n",
    "\n",
    "def load_test_bicoding(pos_test_fa,neg_test_fa):\n",
    "\tdata_pos_test = []\n",
    "\tdata_neg_test = []\n",
    "\t\n",
    "\tdata_pos_test = load_data_bicoding(pos_test_fa)\n",
    "\tdata_neg_test = load_data_bicoding(neg_test_fa)\n",
    "\t\n",
    "\tX_test = np.array([_ for _ in data_pos_test] + [_ for _ in data_neg_test])\n",
    "\ty_test = np.array([1 for _ in data_pos_test] + [0 for _ in data_neg_test])\n",
    "\t\n",
    "\treturn X_test, y_test\n",
    "\n",
    "\n",
    "def load_in_torch_fmt(X_train, y_train,vec_len):\n",
    "\tX_train = X_train.reshape(X_train.shape[0], int(X_train.shape[1]/vec_len), vec_len)\n",
    "\tX_train = torch.from_numpy(X_train).float()\n",
    "\ty_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "\t\n",
    "\treturn X_train, y_train\n",
    "\n",
    "\t\t\t\n",
    "def chunkIt(seq, num): \n",
    "\tavg = len(seq) / float(num)\n",
    "\tout = []\n",
    "\tlast = 0.0\n",
    "\t\n",
    "\twhile last < len(seq):\n",
    "\t\tout.append(seq[int(last):int(last + avg)])\n",
    "\t\tlast += avg\n",
    "\t\t\n",
    "\treturn out\n",
    "\n",
    "def shuffleData(X, y):\n",
    "\tindex = [i for i in range(len(X))]\n",
    "\t# np.random.seed(42)\n",
    "\trandom.shuffle(index)\n",
    "\tnew_X = X[index]\n",
    "\tnew_y = y[index]\n",
    "\treturn new_X, new_y\n",
    "\n",
    "def round_pred(pred):\n",
    "\tlist_result = []\n",
    "\tfor i in pred:\n",
    "\t\tif i >0.5:\n",
    "\t\t\tlist_result.append(1)\n",
    "\t\telif i <=0.5:\n",
    "\t\t\tlist_result.append(0)\n",
    "\treturn torch.tensor(list_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN4_LSTM(nn.Module):\n",
    "\t\tdef __init__(self,kernel_size):\n",
    "\t\t\t\tsuper(CNN4_LSTM, self).__init__()\n",
    "\t\t\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=wordvec_len,\n",
    "\t\t\t\t\t\t\t\tout_channels=256,\n",
    "\t\t\t\t\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\t\t\t\t\tpadding = int(kernel_size/2)),\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\tnn.BatchNorm1d(256),\n",
    "\t\t\t\t\t\tnn.Dropout()\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.conv2 = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=256,\n",
    "\t\t\t\t\t\t\t\tout_channels=256,\n",
    "\t\t\t\t\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\tnn.BatchNorm1d(256),\n",
    "\t\t\t\t\t\tnn.Dropout()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.conv3 = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=256,\n",
    "\t\t\t\t\t\t\t\tout_channels=256,\n",
    "\t\t\t\t\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\tnn.ReLU(),  \n",
    "\t\t\t\t\t\tnn.BatchNorm1d(256),\n",
    "\t\t\t\t\t\tnn.Dropout()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.conv4 = nn.Sequential( \n",
    "\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=256,\n",
    "\t\t\t\t\t\t\t\tout_channels=256, \n",
    "\t\t\t\t\t\t\t\tkernel_size = kernel_size,\n",
    "\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\tnn.ReLU(),\n",
    "\n",
    "\t\t\t\t\t\tnn.BatchNorm1d(256),\n",
    "\t\t\t\t\t\tnn.Dropout()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.lstm = torch.nn.LSTM(256, 32, 2, batch_first=True, bidirectional=True) #\n",
    "\t\t\t\n",
    "\t\t\t\tself.fc_task = nn.Sequential(\n",
    "\t\t\t\t\tnn.Linear(32*2, 32),\n",
    "\t\t\t\t\tnn.Dropout(0.5),\n",
    "\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\tnn.Linear(32, 2),\n",
    "\t\t\t\t)\n",
    "\t\t\t\tself.classifier = nn.Linear(2, 1)\n",
    "\n",
    "\t\t\t\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\t\th0 = Variable(torch.zeros(2 * 2, x.size(0), 32)).to(device)#----cuda ##\n",
    "\t\t\t\tc0 = Variable(torch.zeros(2 * 2, x.size(0), 32)).to(device)#----cuda #3\n",
    "\n",
    "\t\t\t\tx = x.transpose(1, 2)\n",
    "\t\t\t\tx = self.conv1(x) \n",
    "\t\t\t\tx = self.conv2(x)\n",
    "\t\t\t\tx = self.conv3(x)\n",
    "\t\t\t\tx = self.conv4(x)\n",
    "\t\t\t\tx = x.transpose(1, 2)\n",
    "\t\t\t\t#rnn layer\n",
    "\t\t\t\tout, _ = self.lstm(x, (h0, c0))\n",
    "\t\t\t\n",
    "\t\t\t\treduction_feature = self.fc_task(torch.mean(out, 1))\n",
    "\n",
    "\t\t\t\trepresentation = reduction_feature\n",
    "\t\t\t\tlogits_clsf = self.classifier(representation)\n",
    "\t\t\t\tlogits_clsf1 = logits_clsf\n",
    "\t\t\t\tlogits_clsf = torch.sigmoid(logits_clsf)\n",
    "\t\t\t\treturn logits_clsf, representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN4_LSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(4, 256, kernel_size=(10,), stride=(1,), padding=(5,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(256, 256, kernel_size=(10,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(256, 256, kernel_size=(10,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv1d(256, 256, kernel_size=(10,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc_task): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ten/opt/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 97.34566, Training set accuracy: 757/1425 (53.123%)\n",
      "Valid set: Average loss: 103.3581, Accuracy: 102/159 (64.151%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t# Hyper Parameters------------------>>\n",
    "\t\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tdevice_cpu = torch.device(\"cpu\") \n",
    "\twordvec_len =4\n",
    "\t\n",
    "\tk_folds = 10\n",
    "\tEPOCH = 1\n",
    "\tBATCH_SIZE = 256\n",
    "\tLR = 0.01 \n",
    "\tkernel_size = 10\n",
    "\t\n",
    "\t#!!! Model\n",
    "\tnet = CNN4_LSTM(kernel_size =kernel_size).to(device)\n",
    "\tprint(net)  # net architecture\n",
    "\n",
    "\ttrainning_result = []\n",
    "\tvalidation_result = []\n",
    "\ttesting_result = []\n",
    "\t\n",
    "\t\n",
    "\ttrain_loss_sum, valid_loss_sum, test_loss_sum= 0, 0, 0\n",
    "\ttrain_acc_sum , valid_acc_sum , test_acc_sum = 0, 0, 0\n",
    "\t\n",
    "\ttest_acc = []  \n",
    "\ttest_losses = [] \n",
    "\n",
    "\tall_index = ['0']\n",
    "\tfor index_fold in all_index:  \n",
    "\t\tspecies = '880'\n",
    "\t\ttrain_pos_fa = './{0}_data/{0}_positive_train.fa'.format(species)\n",
    "\t\ttrain_neg_fa = './{0}_data/{0}_negative_train.fa'.format(species)\n",
    "\t\t\t\n",
    "\t\tX_train, y_train, X_valid, y_valid = load_train_val_bicoding(train_pos_fa,train_neg_fa)\n",
    "\n",
    "\t\tX_train, y_train = load_in_torch_fmt(X_train, y_train,wordvec_len)\n",
    "\t\tX_valid, y_valid = load_in_torch_fmt(X_valid, y_valid,wordvec_len)\n",
    "\t\n",
    "\t\ttrain_loader = Data.DataLoader(Data.TensorDataset(X_train,y_train), BATCH_SIZE, shuffle = False)\n",
    "\t\tval_loader = Data.DataLoader(Data.TensorDataset(X_valid, y_valid), BATCH_SIZE, shuffle = False)\n",
    "\t\n",
    "\t\tmodel = net\n",
    "\t\n",
    "\t\tcriterion = nn.BCELoss(size_average=False)\n",
    "\t\toptimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n",
    "\n",
    "\t\ttrain_losses = []\n",
    "\t\tval_losses = []\n",
    "\t\n",
    "\t\ttrain_acc = []\n",
    "\t\tval_acc = []\n",
    "\t\n",
    "\t\tbest_acc = 0\n",
    "\t\tpatience = 0\n",
    "\t\tpatience_limit = 30\n",
    "\t\n",
    "\t\tepoch_list = [] #统计用到的epoch，用于画图\n",
    "\t\tfor epoch in range(EPOCH):\n",
    "\t\t\t\n",
    "\t\t\tmodel.train()\n",
    "\t\t\tcorrect = 0   \t\t\n",
    "\t\t\ttrain_loss = 0\n",
    "\t\t\tfor step, (train_x, train_y) in enumerate(train_loader):\n",
    "\t\t\t\t\n",
    "\t\t\t\ttrain_x = Variable(train_x, requires_grad=False).to(device)\n",
    "\t\t\t\ttrain_y = Variable(train_y, requires_grad=False).to(device)\n",
    "\n",
    "\t\t\t\tfx, presention = model(train_x)\n",
    "\n",
    "\t\t\t\tloss = criterion(fx.squeeze(), train_y.type(torch.FloatTensor).to(device))\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tpred = round_pred(fx.data.cpu().numpy()).to(device)\n",
    "\t\t\t\tcorrect += pred.eq(train_y.view_as(pred)).sum().item()\n",
    "\t\t\t\t\n",
    "\t\t\t\ttrain_loss += loss.item() * len(train_y)\n",
    "\t\t\t\tpred_prob = fx\n",
    "\t\t\t\t\n",
    "\t\t\t\tif (step+1) % 10 == 0:\n",
    "\t\t\t\t# 每10个batches打印一次loss\n",
    "\t\t\t\t\tprint ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f'%(epoch + 1, EPOCH, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstep + 1, len(X_train)//BATCH_SIZE, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tloss.item()))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\n",
    "\t\t\taccuracy = 100.*correct/len(X_train)\n",
    "\t\t\t#统计：Epoch: 1, Loss: 0.64163, Training set accuracy: 908/1426 (63.675%)\n",
    "\t\t\tprint('Epoch: {}, Loss: {:.5f}, Training set accuracy: {}/{} ({:.3f}%)'.format(\n",
    "\t\t\t\tepoch + 1, loss.item(), correct, len(X_train), accuracy))\n",
    "\t\t\ttrain_acc.append(accuracy)\n",
    "\t\t\t\n",
    "\t\t\t# 每个epoch计算测试集accuracy\n",
    "\t\t\tmodel.eval() #!!! Valid\n",
    "\t\t\tval_loss = 0\n",
    "\t\t\tcorrect = 0\n",
    "\t\t\trepres_list_valid , label_list_valid = [],[]\n",
    "\t\t\t\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor step, (valid_x, valid_y) in enumerate(val_loader):     \n",
    "\t\t\t\t\tvalid_x = Variable(valid_x, requires_grad=False).to(device)  \n",
    "\t\t\t\t\tvalid_y = Variable(valid_y, requires_grad=False).to(device) \n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\ty_hat_val, presention_valid = model(valid_x)\n",
    "\t\t\t\t\tloss = criterion(y_hat_val.squeeze(), valid_y.type(torch.FloatTensor).to(device)).item()  #BE\n",
    "\t\t\t\t\tval_loss += loss * len(valid_y)            \t\t\t\t\t\t\t\t\t   #Cross\n",
    "\t\t\t\t\tpred_val = round_pred(y_hat_val.data.cpu().numpy()).to(device)    \t\t\t\t\t\t\t   #BE\n",
    "\t\t\t\t\tcorrect += pred_val.eq(valid_y.view_as(pred_val)).sum().item()\n",
    "\n",
    "\t\t\tval_losses.append(val_loss/len(X_valid)) # all loss / all sample\n",
    "\t\t\taccuracy = 100.*correct/len(X_valid)\n",
    "\t\t\t#统计：Valid set: Average loss: 410.8327, Accuracy: 148/158 (93.671%)\n",
    "\t\t\tprint('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "\t\t\t\tval_loss/len(X_valid), correct, len(X_valid), accuracy))\n",
    "\t\t\tval_acc.append(accuracy)\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
